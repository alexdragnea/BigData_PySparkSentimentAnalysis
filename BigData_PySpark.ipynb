{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6efd439c-3268-4e0b-bed4-36656ba18fec",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3791343f-d4ae-44cd-a782-adcc33b15009",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "from pyspark.ml.feature import Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import PipelineModel\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import praw\n",
    "import os\n",
    "\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80bf361-40d8-480d-8afb-f890b3df2053",
   "metadata": {},
   "source": [
    "## Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfd7f01d-44de-4d8c-a3e6-f418e1e6e9a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/12/10 19:10:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Initialize a Spark session\n",
    "spark = SparkSession.builder\\\n",
    "            .master(\"local[16]\")\\\n",
    "            .appName(\"LR_Twitter\")\\\n",
    "            .config(\"spark.driver.memory\", \"6g\")\\\n",
    "            .config(\"spark.executor.memory\", \"6g\")\\\n",
    "            .config(\"spark.driver.maxResultSize\", \"3g\")\\\n",
    "            .getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"OFF\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39c21b0-cd0a-49e0-bbca-94c2d23bb22b",
   "metadata": {},
   "source": [
    "## Cleaning Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff5b37b-63b3-46e2-8bde-06ee20779edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    negations_dic = {\n",
    "        \"isn't\": \"is not\", \"aren't\": \"are not\", \"wasn't\": \"was not\", \"weren't\": \"were not\",\n",
    "        \"haven't\": \"have not\", \"hasn't\": \"has not\", \"hadn't\": \"had not\", \"won't\": \"will not\",\n",
    "        \"wouldn't\": \"would not\", \"don't\": \"do not\", \"doesn't\": \"does not\", \"didn't\": \"did not\",\n",
    "        \"can't\": \"cannot\", \"couldn't\": \"could not\", \"shouldn't\": \"should not\", \"mightn't\": \"might not\",\n",
    "        \"mustn't\": \"must not\"\n",
    "    }\n",
    "    \n",
    "    combined_pat = r'|'.join((r'@[A-Za-z0-9_]+', r'https?://[^ ]+', r'www.[^ ]+'))\n",
    "    www_pat = r'www.[^ ]+'\n",
    "    neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "    tok = WordPunctTokenizer()\n",
    "\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    try:\n",
    "        bom_removed = souped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        bom_removed = souped\n",
    "    stripped = re.sub(combined_pat, '', bom_removed)\n",
    "    stripped = re.sub(www_pat, '', stripped)\n",
    "    lower_case = stripped.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    words = [x for x in tok.tokenize(letters_only) if len(x) > 1]\n",
    "    return \" \".join(words).strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb6769c-cdad-4bb9-93be-6c47c759ab04",
   "metadata": {},
   "source": [
    "## Cleaning Dataset Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49552b9-8b64-4346-adad-46b8063de6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), False),\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"user\", StringType(), True),\n",
    "    StructField(\"text\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Load data with the defined schema\n",
    "df = spark.read.csv(\"training.1600000.processed.noemoticon.csv\", header=None, schema=schema, encoding='ISO-8859-1')\n",
    "\n",
    "print(\"========================Before cleaning========================\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Apply the cleaning function to the \"text\" column\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "df = df.withColumn(\"text\",clean_text_udf(df[\"text\"]))\n",
    "\n",
    "# Map sentiment (0 to 0 and 4 to 1)\n",
    "df = df.withColumn(\"target\", when(df[\"target\"] == 0, 0).otherwise(1))\n",
    "\n",
    "# Select the relevant columns (index, sentiment, and cleaned text)\n",
    "df = df.select(\"text\", \"target\")\n",
    "\n",
    "print(\"========================After cleaning========================\")\n",
    "\n",
    "df.show()\n",
    "\n",
    "# Convert the Spark DataFrame to a pandas DataFrame\n",
    "df_pandas = df.toPandas()\n",
    "\n",
    "# Save the pandas DataFrame to a CSV file\n",
    "df_pandas.to_csv(\"clean_tweet.csv\", index=True, encoding='utf-8')\n",
    "\n",
    "st = datetime.utcnow()\n",
    "print('Cleaning time:', datetime.utcnow() - st)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41da3b70-5a0f-456f-bf8d-0f60a0526e2b",
   "metadata": {},
   "source": [
    "## Load Cleaned CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9659109c-deca-4595-a79c-68a5ce98fd2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 1596041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "path = \"clean_tweet.csv\"\n",
    "schema = StructType([\n",
    "    StructField(\"_\", StringType(), True),  # Skip the first column\n",
    "    StructField(\"text\", StringType(), True),\n",
    "    StructField(\"target\", IntegerType(), True)\n",
    "])\n",
    "df = spark.read.csv(path, inferSchema=True, header=False, schema=schema).na.drop()\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "print(f\"Number of rows in the DataFrame: {df.count()}\")\n",
    "\n",
    "df = df.select(\"text\", \"target\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d47d3-b801-47ee-af75-c1b3960a0ae0",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49711233-c4ce-42a1-8043-74cc597f589c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'regParam'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Pipeline(stages\u001b[38;5;241m=\u001b[39mtokenizer \u001b[38;5;241m+\u001b[39m stopword_remover \u001b[38;5;241m+\u001b[39m ngrams \u001b[38;5;241m+\u001b[39m cv \u001b[38;5;241m+\u001b[39m idf \u001b[38;5;241m+\u001b[39m assembler \u001b[38;5;241m+\u001b[39m label_stringIdx \u001b[38;5;241m+\u001b[39m selector \u001b[38;5;241m+\u001b[39m [tvs])\n\u001b[1;32m     47\u001b[0m st \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[0;32m---> 49\u001b[0m pipelineFit \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_trigrams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfit(train_set)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining time:\u001b[39m\u001b[38;5;124m'\u001b[39m, datetime\u001b[38;5;241m.\u001b[39mutcnow() \u001b[38;5;241m-\u001b[39m st)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Get the current directory\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 35\u001b[0m, in \u001b[0;36mbuild_trigrams\u001b[0;34m(inputCol, n)\u001b[0m\n\u001b[1;32m     30\u001b[0m selector \u001b[38;5;241m=\u001b[39m [ChiSqSelector(numTopFeatures\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m14\u001b[39m, featuresCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawFeatures\u001b[39m\u001b[38;5;124m'\u001b[39m, outputCol\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeatures\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     32\u001b[0m lr \u001b[38;5;241m=\u001b[39m [LogisticRegression(regParam\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m)]\n\u001b[1;32m     34\u001b[0m paramGrid \u001b[38;5;241m=\u001b[39m (ParamGridBuilder() \n\u001b[0;32m---> 35\u001b[0m              \u001b[38;5;241m.\u001b[39maddGrid(\u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregParam\u001b[49m, [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.01\u001b[39m]) \n\u001b[1;32m     36\u001b[0m              \u001b[38;5;241m.\u001b[39maddGrid(lr\u001b[38;5;241m.\u001b[39mfitIntercept, [\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m]) \n\u001b[1;32m     37\u001b[0m              \u001b[38;5;241m.\u001b[39mbuild())\n\u001b[1;32m     39\u001b[0m tvs \u001b[38;5;241m=\u001b[39m TrainValidationSplit(estimator\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m     40\u001b[0m                            estimatorParamMaps\u001b[38;5;241m=\u001b[39mparamGrid,\n\u001b[1;32m     41\u001b[0m                            evaluator\u001b[38;5;241m=\u001b[39mevaluator,\n\u001b[1;32m     42\u001b[0m                            trainRatio\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Pipeline(stages\u001b[38;5;241m=\u001b[39mtokenizer \u001b[38;5;241m+\u001b[39m stopword_remover \u001b[38;5;241m+\u001b[39m ngrams \u001b[38;5;241m+\u001b[39m cv \u001b[38;5;241m+\u001b[39m idf \u001b[38;5;241m+\u001b[39m assembler \u001b[38;5;241m+\u001b[39m label_stringIdx \u001b[38;5;241m+\u001b[39m selector \u001b[38;5;241m+\u001b[39m [tvs])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'regParam'"
     ]
    }
   ],
   "source": [
    "# Split dataset into a training set and a test set\n",
    "(train_set, test_set) = df.randomSplit([0.80, 0.20], seed=2000)\n",
    "\n",
    "# Define a function to buildthe pipeline with trigrams\n",
    "def build_trigrams(inputCol=[\"text\", \"target\"], n=3):\n",
    "    tokenizer = [Tokenizer(inputCol=\"text\", outputCol=\"words\")]\n",
    "\n",
    "    ngrams = [\n",
    "        NGram(n=i, inputCol=\"words\", outputCol=\"{0}_grams\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    cv = [\n",
    "        CountVectorizer(vocabSize=2**14, inputCol=\"{0}_grams\".format(i),\n",
    "            outputCol=\"{0}_tf\".format(i))\n",
    "        for i in range(1, n + 1)\n",
    "    ]\n",
    "\n",
    "    idf = [IDF(inputCol=\"{0}_tf\".format(i), outputCol=\"{0}_tfidf\".format(i), minDocFreq=5) for i in range(1, n + 1)]\n",
    "\n",
    "    stopword_remover = [StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\")]\n",
    "\n",
    "    assembler = [VectorAssembler(\n",
    "        inputCols=[\"{0}_tfidf\".format(i) for i in range(1, n + 1)],\n",
    "        outputCol=\"rawFeatures\"\n",
    "    )]\n",
    "\n",
    "    label_stringIdx = [StringIndexer(inputCol=\"target\", outputCol=\"label\")]\n",
    "\n",
    "    selector = [ChiSqSelector(numTopFeatures=2**14, featuresCol='rawFeatures', outputCol=\"features\")]\n",
    "\n",
    "    lr = [LogisticRegression(regParam=0.005)]\n",
    "\n",
    "    return Pipeline(stages=tokenizer + stopword_remover + ngrams + cv + idf + assembler + label_stringIdx + selector + lr)\n",
    "\n",
    "\n",
    "st = datetime.utcnow()\n",
    "\n",
    "pipelineFit = build_trigrams().fit(train_set)\n",
    "print('Training time:', datetime.utcnow() - st)\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Construct the model path in the current directory\n",
    "model_path = os.path.join(current_directory, \"lr_mode\")  # Replace \"your_model_name\" with the desired model name\n",
    "# Save the trained model\n",
    "pipelineFit.write().overwrite().save(model_path)\n",
    "\n",
    "#Get Metrics\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "predictions = pipelineFit.transform(test_set)\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Get the ROC AUC\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
    "roc_auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
    "\n",
    "# Print the results\n",
    "print(\"ROC AUC:\", roc_auc)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce05f41-2ce7-421c-96a9-11dada805874",
   "metadata": {},
   "source": [
    "## Distribution of sentiments in labeled dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54151405-3338-46a3-8670-c106ae7ea47b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of positive and negative sentiments in the labeled dataset\n",
    "positive_count = df[df['target'] == 1].count()\n",
    "negative_count = df[df['target'] == 0].count()\n",
    "\n",
    "# Create a bar chart to show the distribution\n",
    "sentiments = ['Positive', 'Negative']\n",
    "counts = [positive_count, negative_count]\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(sentiments, counts, color=['green', 'red'])\n",
    "plt.xlabel('Sentiment')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of Sentiments in labeled dataset')\n",
    "plt.show()\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788195ec-a62a-4eff-a481-0ad9a08eaf22",
   "metadata": {},
   "source": [
    "## Real time prediction on Reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5c68eb-2d93-4f24-b0ea-2dfc17304468",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reddit API setup\n",
    "reddit = praw.Reddit(\n",
    "    client_id='mJMczWy81dMME1r9v9OBiQ',\n",
    "    client_secret='tdsaBJ8OiNORXPvbw1jwFkNCaQY3GQ',\n",
    "    user_agent='redit api2',\n",
    "    username='Public-Quantity3513',\n",
    "    password='siCsur-2qujcy-tehmim'\n",
    ")\n",
    "\n",
    "subreddit_name = 'DataScience'\n",
    "\n",
    "\n",
    "# Load the sentiment analysis pipeline model\n",
    "model_path = \"lr_mode\"  # Replace with the actual path\n",
    "pipeline_model = PipelineModel.load(model_path)\n",
    "\n",
    "clean_text_udf = udf(clean_text, StringType())\n",
    "\n",
    "def process_post(post):\n",
    "    post_comment_bodies = []\n",
    "    for comment in post.comments.list():\n",
    "        if isinstance(comment, praw.models.Comment) and not comment.body == '[deleted]':\n",
    "            cleaned_text = clean_text(comment.body)\n",
    "            post_comment_bodies.append(cleaned_text)\n",
    "    return post_comment_bodies\n",
    "\n",
    "def predict_sentiment(comment_bodies):\n",
    "    schema = StructType([StructField(\"text\", StringType(), True)])\n",
    "    df = spark.createDataFrame([(body,) for body in comment_bodies], schema=schema)\n",
    "    predictions = pipeline_model.transform(df)\n",
    "    \n",
    "    # Collect predicted labels\n",
    "    predicted_labels = predictions.select('prediction').collect()\n",
    "    \n",
    "    # Plot the distribution of predicted sentiments\n",
    "    predicted_positive_count = sum(1 for label in predicted_labels if label[0] == 1)\n",
    "    predicted_negative_count = sum(1 for label in predicted_labels if label[0] == 0)\n",
    "\n",
    "    predicted_sentiments = ['Predicted Positive', 'Predicted Negative']\n",
    "    predicted_counts = [predicted_positive_count, predicted_negative_count]\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar(predicted_sentiments, predicted_counts, color=['green', 'red'])\n",
    "    plt.xlabel('Sentiment')\n",
    "    plt.ylabel('Count')\n",
    "    plt.title('Sentiment Prediction Distribution On Reddit Live Data')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "try:\n",
    "    hot_posts = reddit.subreddit(subreddit_name).hot(limit=500)\n",
    "    all_comment_bodies = []\n",
    "\n",
    "    for post in hot_posts:\n",
    "        comment_bodies = process_post(post)\n",
    "        all_comment_bodies.extend(comment_bodies)\n",
    "\n",
    "    # Predict sentiment for all comments\n",
    "    predict_sentiment(all_comment_bodies)\n",
    "\n",
    "    print(\"Sentiment analysis completed for Reddit data\")\n",
    "\n",
    "except praw.exceptions.APIException as e:\n",
    "    print(f\"Reddit API Exception: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Stop Spark session\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa51cf3-18bd-47f5-8e45-64eecc9acea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
